{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz7mOJzl78PS",
        "outputId": "752adbbd-2405-4b2a-efea-85dd47f7beea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# Example 1000-word text (you can replace this with your own text)\n",
        "sample_text = \"\"\"Artificial Intelligence (AI) is rapidly transforming industries...\"\"\" * 50  # repeat to simulate ~1000 words\n",
        "\n",
        "# Load encoder for GPT-4o-mini (uses same encoding as cl100k_base)\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "tokens = encoding.encode(sample_text)\n",
        "token_count = len(tokens)\n",
        "\n",
        "print(f\"Total tokens: {token_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YO2HlHxl8MoC",
        "outputId": "5c3940d9-0ab3-48e2-df89-ae98f1c19935"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost per 1K tokens in USD (as of Oct 2025)\n",
        "INPUT_COST = 0.00015\n",
        "OUTPUT_COST = 0.00060\n",
        "\n",
        "# Suppose prompt = 1000 words (~750 tokens), response = 200 tokens\n",
        "prompt_tokens = token_count\n",
        "response_tokens = 200\n",
        "\n",
        "prompt_cost = (prompt_tokens / 1000) * INPUT_COST\n",
        "response_cost = (response_tokens / 1000) * OUTPUT_COST\n",
        "total_cost = prompt_cost + response_cost\n",
        "\n",
        "print(f\"Prompt tokens: {prompt_tokens}\")\n",
        "print(f\"Response tokens: {response_tokens}\")\n",
        "print(f\"Prompt cost: ${prompt_cost:.5f}\")\n",
        "print(f\"Response cost: ${response_cost:.5f}\")\n",
        "print(f\"Total estimated cost: ${total_cost:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IbFxIcT8c88",
        "outputId": "9ed03947-32a0-45c7-cba1-251f0a347100"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 550\n",
            "Response tokens: 200\n",
            "Prompt cost: $0.00008\n",
            "Response cost: $0.00012\n",
            "Total estimated cost: $0.00020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_cost(prompt_tokens, response_tokens, input_rate, output_rate):\n",
        "    return (prompt_tokens / 1000 * input_rate) + (response_tokens / 1000 * output_rate)\n",
        "\n",
        "models = {\n",
        "    \"gpt-4o-mini\": (0.00015, 0.00060),\n",
        "    \"gpt-4o\": (0.005, 0.015),\n",
        "    \"gpt-3.5-turbo\": (0.0005, 0.0015)\n",
        "}\n",
        "\n",
        "for model, (in_rate, out_rate) in models.items():\n",
        "    cost = estimate_cost(prompt_tokens, response_tokens, in_rate, out_rate)\n",
        "    print(f\"{model:15} → ${cost:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogYVJQk68nfd",
        "outputId": "15938f21-411e-4e24-f669-3ff526618d7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-4o-mini     → $0.00020\n",
            "gpt-4o          → $0.00575\n",
            "gpt-3.5-turbo   → $0.00057\n"
          ]
        }
      ]
    }
  ]
}