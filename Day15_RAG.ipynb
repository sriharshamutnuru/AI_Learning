{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm3xEzCNfpQQFF0gdue3si",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sriharshamutnuru/AI_Learning/blob/main/Day15_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "7S28W3ajJ__n",
        "outputId": "122f284d-e38c-4413-898c-d777ac683235"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n# üß† Retrieval-Augmented Generation (RAG)\n\n**RAG (Retrieval-Augmented Generation)** is an architecture that combines vector-based information retrieval with large language model generation.\nIt ensures that responses are *grounded* in external data, allowing models to stay up-to-date without retraining.\n\n---\n## üß© Core Components\n\n1. **Document Ingestion** ‚Äì Load enterprise or knowledge base documents (PDFs, HTML, text)\n2. **Chunking** ‚Äì Split large text into smaller, retrievable segments\n3. **Embeddings** ‚Äì Convert chunks into numerical vector representations using embedding models\n4. **Vector Database** ‚Äì Store embeddings (FAISS, Chroma, Azure Cognitive Search)\n5. **Retriever** ‚Äì Find top relevant chunks for a user query\n6. **LLM** ‚Äì Combine query + retrieved chunks ‚Üí generate grounded answer\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n```mermaid\ngraph TD\nA[Document Source] --> B[Text Chunking]\nB --> C[Embedding Model]\nC --> D[Vector Database]\nD --> E[Retriever]\nE --> F[LLM Generation]\nF --> G[Final Grounded Response]\n```\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n| Aspect | Traditional LLM | RAG |\n|--------|-----------------|-----|\n| **Knowledge Source** | Pretrained data only | External documents |\n| **Update Cycle** | Requires retraining | Dynamic updates via reindexing |\n| **Accuracy** | May hallucinate | Grounded, factual |\n| **Traceability** | No citations | Source-linked responses |\n| **Cost** | High (fine-tuning) | Low (embedding + retrieval) |\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n### ‚úçÔ∏è Short Writeup (Checkpoint)\nRAG (Retrieval-Augmented Generation) enhances LLMs with external data retrieval. Instead of fine-tuning, it retrieves relevant information from a vector database and injects it into the LLM prompt. This allows updatable, grounded, and factual responses while maintaining low operational cost. RAG is ideal for enterprise systems like knowledge assistants, policy chatbots, or compliance tools where traceability and context are critical.\n\n**Key Benefit:** Dynamic knowledge ‚Üí factual accuracy ‚Üí explainable outputs.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n| Concept | Status |\n|----------|---------|\n| RAG Definition | ‚úÖ Understood |\n| Architecture Flow | ‚úÖ Diagrammed |\n| When to Use | ‚úÖ Clarified |\n| Writeup | ‚úÖ Complete |\n| Skill Acquired | RAG System Design Foundations |\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ============================================================\n",
        "# üìò Day 15 ‚Äî Retrieval-Augmented Generation (RAG) Overview\n",
        "# ============================================================\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# --- Section 1: RAG Overview ---\n",
        "rag_intro = \"\"\"\n",
        "# üß† Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "**RAG (Retrieval-Augmented Generation)** is an architecture that combines vector-based information retrieval with large language model generation.\n",
        "It ensures that responses are *grounded* in external data, allowing models to stay up-to-date without retraining.\n",
        "\n",
        "---\n",
        "## üß© Core Components\n",
        "\n",
        "1. **Document Ingestion** ‚Äì Load enterprise or knowledge base documents (PDFs, HTML, text)\n",
        "2. **Chunking** ‚Äì Split large text into smaller, retrievable segments\n",
        "3. **Embeddings** ‚Äì Convert chunks into numerical vector representations using embedding models\n",
        "4. **Vector Database** ‚Äì Store embeddings (FAISS, Chroma, Azure Cognitive Search)\n",
        "5. **Retriever** ‚Äì Find top relevant chunks for a user query\n",
        "6. **LLM** ‚Äì Combine query + retrieved chunks ‚Üí generate grounded answer\n",
        "\n",
        "---\n",
        "\"\"\"\n",
        "display(Markdown(rag_intro))\n",
        "\n",
        "# --- Section 2: Mermaid Diagram ---\n",
        "rag_diagram = \"\"\"\n",
        "```mermaid\n",
        "graph TD\n",
        "A[Document Source] --> B[Text Chunking]\n",
        "B --> C[Embedding Model]\n",
        "C --> D[Vector Database]\n",
        "D --> E[Retriever]\n",
        "E --> F[LLM Generation]\n",
        "F --> G[Final Grounded Response]\n",
        "```\n",
        "\"\"\"\n",
        "display(Markdown(rag_diagram))\n",
        "\n",
        "# --- Section 3: Comparison Table ---\n",
        "comparison = \"\"\"\n",
        "| Aspect | Traditional LLM | RAG |\n",
        "|--------|-----------------|-----|\n",
        "| **Knowledge Source** | Pretrained data only | External documents |\n",
        "| **Update Cycle** | Requires retraining | Dynamic updates via reindexing |\n",
        "| **Accuracy** | May hallucinate | Grounded, factual |\n",
        "| **Traceability** | No citations | Source-linked responses |\n",
        "| **Cost** | High (fine-tuning) | Low (embedding + retrieval) |\n",
        "\"\"\"\n",
        "display(Markdown(comparison))\n",
        "\n",
        "# --- Section 4: Sample Writeup ---\n",
        "rag_writeup = \"\"\"\n",
        "### ‚úçÔ∏è Short Writeup (Checkpoint)\n",
        "RAG (Retrieval-Augmented Generation) enhances LLMs with external data retrieval. Instead of fine-tuning, it retrieves relevant information from a vector database and injects it into the LLM prompt. This allows updatable, grounded, and factual responses while maintaining low operational cost. RAG is ideal for enterprise systems like knowledge assistants, policy chatbots, or compliance tools where traceability and context are critical.\n",
        "\n",
        "**Key Benefit:** Dynamic knowledge ‚Üí factual accuracy ‚Üí explainable outputs.\n",
        "\"\"\"\n",
        "display(Markdown(rag_writeup))\n",
        "\n",
        "# --- Section 5: Checkpoint Summary ---\n",
        "summary = \"\"\"\n",
        "| Concept | Status |\n",
        "|----------|---------|\n",
        "| RAG Definition | ‚úÖ Understood |\n",
        "| Architecture Flow | ‚úÖ Diagrammed |\n",
        "| When to Use | ‚úÖ Clarified |\n",
        "| Writeup | ‚úÖ Complete |\n",
        "| Skill Acquired | RAG System Design Foundations |\n",
        "\"\"\"\n",
        "display(Markdown(summary))"
      ]
    }
  ]
}