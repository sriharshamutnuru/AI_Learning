{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213,
     "referenced_widgets": [
      "4f06913593e94501b5c80b67fc5999c7",
      "6e4dd13232b2471e9f81f2b7c8213b20",
      "acbf9ca11d0c48b6ab7e94df88e7de75",
      "cc447181ba784769aac58c6ead10fca9",
      "642caa0fce7f49589922063881d7ac56",
      "0ebf03a06de044cb973f6d9332a7dc06",
      "fb66555da74647ddac011e682e33dc1f",
      "c0395b230e004d21a907fb72e8c7ee19",
      "35436b92fb884168bc02cfc018dd49ac",
      "6b7f3b7d5deb45da9e297f8b5184e8eb",
      "c1bdeb44f7a44d458718be671c000a18",
      "be45a34963994a52842337601aaafb00",
      "037becb3915a45dfa9b41e00e786dfc0",
      "47166f46f9bd4399b44fdb032ede89fa",
      "127c7c11d9e84351935b425fce7af59b",
      "e138bf03dce04839b9c80c4d600bbdfc",
      "f07906a08add4abab8433cd42a5cb676",
      "0aa22a5853b44d6eba51f86dbe2f62b7",
      "dd554b41a7dc4371a87d79798735c2df",
      "713fb749ff454b93951c1c3b9cb656b2",
      "a832e6700c76415eb3444b4c43cbc307",
      "bd24e70e123740bba90e45ab6fa2d979",
      "4c568c80a9e149bd9e98d06142d38fdb",
      "5505961f21a744ea91c0b4a0960def77",
      "8e7f7f8e678c42f88ebd1cbc8f528c8d",
      "204b38e52b27471f9a63cfd4585f2b44",
      "989732df106a4de5bdffb763774e04ea",
      "3fa1194d3e364e2d9a154af515e9e5e3",
      "066f52ac686a4871aeb56ab27096086c",
      "e0eaf881937d49a0b7f77e99f061d47e",
      "861c9149d00d47eda85e8eaec8c45dc6",
      "442597fb74054847b6fe087379f7d051",
      "cab08cdec7704d27ac81a2ad3dc7a5ed",
      "852d6d9a0bf644659e9541e90276fa28",
      "e0f042aa86474a08a2acb84b1438b021",
      "75df98f0cc3645d58a54c44f82763af7",
      "051981a16d234f18b33c691b50ccc4ed",
      "720c76bbc7054e129dba91cab0d3dba4",
      "e4ccc6941beb40ab8e3a1c1684f1cb91",
      "28e714c86a244d72af04c248e0dca171",
      "9c8125ecb4c74a9bb4d68f632d13b947",
      "ad55baad80684a4f9ccbea935a8dc4f0",
      "5557ab75c1c948e19bb0112ed9b698e3",
      "d513596919b14a90a8284438398e6348",
      "7256fb311f204e8cb58a2199e5cd86aa",
      "0bc73aaa79134e0892f210025ad70693",
      "b2b24293e12645a68b9deadb476820f2",
      "a5b8f33bf00a4872b7ad51ed46a2c76e",
      "543d8a8c5dff440fa07e18d57c08f189",
      "d4a31b89d71a4e39a8d2d26d7a44b340",
      "9d1e94ff97724182853ee38ea62c843e",
      "8858cb817feb4c55a7b268155cdb288d",
      "f0702ef3785f449cb514ab2233bc8a32",
      "392f8998afd549eb9945b89be3d7ce2b",
      "562471be66454d9d88742be64af6129f"
     ]
    },
    "id": "i1Eb1KYZ6HEh",
    "outputId": "fb68a692-81ef-4f86-c568-291ba728a238"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f06913593e94501b5c80b67fc5999c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be45a34963994a52842337601aaafb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c568c80a9e149bd9e98d06142d38fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852d6d9a0bf644659e9541e90276fa28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7256fb311f204e8cb58a2199e5cd86aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[41762,   364,   389,  3665,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "['Transform', 'ers', 'Ġare', 'Ġpowerful', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer(\"Transformers are powerful!\", return_tensors=\"pt\")\n",
    "print(tokens)\n",
    "print(tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "c6b0dfc9531e4486a8a7f7e970c1c0b6",
      "153c6da9b8f44b03ad6f92bc3e1dcd8d",
      "24cc87cb5eea43f39023bedff5231320",
      "275922002ed44a7396b43d8207d38734",
      "280c8d4c74354f7d880920d7f2bfb0b7",
      "8cca3f4eef554d66bfda58c57622c852",
      "bbc61c7902d549b7ba2c4d65704ab997",
      "d50c2b16f00d4a28a6669f6fe0393542",
      "230fac5b367546ab867ccd6d5dced3dc",
      "7e073c0b94194c1b81669b45259b4fe3",
      "133f041bd0544d1caa2c7deab76e88f9",
      "77b85c6a17aa4cd5b754c6b9ff6001cf",
      "0fd1d351796c406d9b09674c8516818b",
      "26306650934c4987810113294ccd1f14",
      "cac482263e9f4e9db350cbb746101523",
      "104c3b81aaf8443c8874877c38023ea9",
      "4763d4f458d3470a90c15f4f0ce38ac3",
      "40b20b7eac4b467a82506fff339aa99c",
      "766b9109fe27429db13e9d7181e9e30f",
      "94c25f1f135147a0a5425c38395f1383",
      "a35c64d573474ecab334569053bd08b0",
      "8fd8876e6ea14dc8a89fcbe6f5ef1942"
     ]
    },
    "id": "U8jvASz784yh",
    "outputId": "312b2ff0-5f60-46ff-a5a0-c5fd1e084a1d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b0dfc9531e4486a8a7f7e970c1c0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b85c6a17aa4cd5b754c6b9ff6001cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the future, artificial intelligence will be used to detect a variety of crimes in order to identify suspects, according to a report from the FBI.\n",
      "\n",
      "The U.S. Justice Department's Cybercrime Laboratory said its analysis and the results of a report prepared by the National Security Agency revealed that the U.S. had already begun to use AI to detect terrorism. The report also stated that the U.S. should not adopt any technology that could make terrorists less likely to carry out attacks like the one in Paris.\n",
      "\n",
      "The report was released in part as part of the FBI's annual Cybersecurity Assessment. The report also said that in the future, artificial intelligence will be used to detect a variety of crimes in order to identify suspects, according to a report from the FBI.\n",
      "\n",
      "The report said the FBI believes that AI could be used to detect terrorist attacks.\n",
      "\n",
      "\"AI is a highly effective tool for the task of screening and detecting potential terrorist threats, and it also enables the FBI to enhance its intelligence community's ability to respond to and disrupt threats,\" the report said.\n",
      "\n",
      "\"In this regard, the report noted that the FBI's research indicates that AI may be used to detect a variety of terrorist threats, including those of the Islamic State, the al-Nusra Front and the\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers --quiet\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "prompt = \"In the future, artificial intelligence will\"\n",
    "output = generator(prompt, max_length=40, num_return_sequences=1)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUvT4m3P9eZJ",
    "outputId": "14fa5bbe-9631-4b5b-e795-17bfb92a14d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Large', 'ĠLanguage', 'ĠModels', 'Ġare', 'Ġamazing', '!']\n",
      "Token IDs: [21968, 15417, 32329, 389, 4998, 0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Large Language Models are amazing!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7D8q8CALhKx",
    "outputId": "bd0b699e-b730-4afd-d638-7a618e1c9c1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI in healthcare is also a great concern. The first priority is to make sure we understand and learn from our research. You can be sure that you'll be able to better understand our work and share your findings with other researchers and clinicians. I hope this article can help you, too.\n",
      "\n",
      "What Is A Artificial Intelligence?\n",
      "\n",
      "A system is a system that creates things that can be made from the data it generates. These things are called \"programmed machine intelligence\", and are called machine learning systems because they process data in a way that can be applied to any part of the system.\n",
      "\n",
      "A machine learning system is basically a computer program that creates or learns about a certain set of data. It's essentially a processor, which gets inputs from the processing system and, when it learns about that set of data, it then reads the data from that processor and can then interpret that data.\n",
      "\n",
      "One of the big challenges in developing a machine learning system is that it's really hard to know what to do with data from a machine. When I say \"programmed,\" I mean the program we're using to make things.\n",
      "\n",
      "In AI, we've got a huge amount of data that we're trying to process. A lot of what we're doing is applying some of\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\",model=\"gpt2\")\n",
    "prompt = \"The future of AI in healthcare is\"\n",
    "output = generator(prompt, max_length=40, num_return_sequences=1)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70JroBOWNIhm",
    "outputId": "a2db4c11-b053-4e70-cb43-99398b035d97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'Ġfuture', 'Ġof', 'ĠAI', 'Ġin', 'Ġhealthcare', 'Ġis']\n",
      "Token IDs: [464, 2003, 286, 9552, 287, 11409, 318]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "text = \"The future of AI in healthcare is\"\n",
    "tokens= tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", ids)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
